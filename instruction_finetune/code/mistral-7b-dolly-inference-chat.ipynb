{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45533,"databundleVersionId":5748852,"sourceType":"competition"},{"sourceId":154410468,"sourceType":"kernelVersion"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q bitsandbytes\n!pip install -q accelerate\n!pip install -q peft","metadata":{"execution":{"iopub.status.busy":"2023-12-14T07:37:28.461287Z","iopub.execute_input":"2023-12-14T07:37:28.461551Z","iopub.status.idle":"2023-12-14T07:38:07.579910Z","shell.execute_reply.started":"2023-12-14T07:37:28.461526Z","shell.execute_reply":"2023-12-14T07:38:07.578827Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport re\nimport string\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig\n)\n\nfrom peft import (\n    get_peft_config,\n    get_peft_model,\n    LoraConfig,\n    TaskType\n)\n\n\nos.environ['TOKENIZERS_PARALLELISM'] = 'false'","metadata":{"execution":{"iopub.status.busy":"2023-12-14T07:38:07.581855Z","iopub.execute_input":"2023-12-14T07:38:07.582186Z","iopub.status.idle":"2023-12-14T07:38:12.879614Z","shell.execute_reply.started":"2023-12-14T07:38:07.582155Z","shell.execute_reply":"2023-12-14T07:38:12.878810Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# General Settings","metadata":{}},{"cell_type":"code","source":"llm_backbone = 'mistralai/Mistral-7B-v0.1'\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-12-14T07:38:12.880705Z","iopub.execute_input":"2023-12-14T07:38:12.881143Z","iopub.status.idle":"2023-12-14T07:38:12.913388Z","shell.execute_reply.started":"2023-12-14T07:38:12.881097Z","shell.execute_reply":"2023-12-14T07:38:12.912535Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Load tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(llm_backbone)\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-12-14T07:38:12.915638Z","iopub.execute_input":"2023-12-14T07:38:12.915932Z","iopub.status.idle":"2023-12-14T07:38:14.232877Z","shell.execute_reply.started":"2023-12-14T07:38:12.915908Z","shell.execute_reply":"2023-12-14T07:38:14.231957Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/967 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f178462470de450a90d8e959870c209c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05d04f85908465295cdc2caa1488645"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"262e8033c9a64f5ca33aabffeac24c9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e51a560dea40459b80cc3af9100fe3fe"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        bnb_config = BitsAndBytesConfig(\n                load_in_4bit=True,\n                bnb_4bit_use_double_quant=True,\n                bnb_4bit_quant_type=\"nf4\",\n                bnb_4bit_compute_dtype=torch.float16\n        )\n        \n        self.backbone =  AutoModelForCausalLM.from_pretrained(\n            llm_backbone,\n            quantization_config=bnb_config,\n            load_in_4bit=True,\n        )\n        \n        self.peft_config = LoraConfig(\n            task_type=TaskType.CAUSAL_LM, \n            inference_mode=False,\n            r=8,\n            lora_alpha=16, \n            lora_dropout=0.05\n        )\n        \n        self.backbone = get_peft_model(self.backbone, self.peft_config)\n        \n        self.backbone.print_trainable_parameters()\n        \n    def forward(self, input_ids, targets):\n        logits = self.backbone(input_ids).logits\n        \n        B, T, C = logits.shape\n    \n        logits = logits[..., :-1, :]\n        logits = logits.reshape(B*T, C)\n        \n        targets = targets[..., 1:]\n        targets = targets.reshape(-1)\n        \n        loss = F.cross_entropy(logits, targets, ignore_index=-1)\n        \n        return logits, loss","metadata":{"execution":{"iopub.status.busy":"2023-12-14T07:38:14.233898Z","iopub.execute_input":"2023-12-14T07:38:14.234185Z","iopub.status.idle":"2023-12-14T07:38:14.244697Z","shell.execute_reply.started":"2023-12-14T07:38:14.234159Z","shell.execute_reply":"2023-12-14T07:38:14.243514Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Load trained weights","metadata":{}},{"cell_type":"code","source":"model = Net()\n\nmodel.load_state_dict(\n    torch.load(\n        '/kaggle/input/mistral-7b-dolly/mistral_7b_dolly.pt', \n        map_location=device\n    ),\n    strict=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-14T07:38:14.245859Z","iopub.execute_input":"2023-12-14T07:38:14.246228Z","iopub.status.idle":"2023-12-14T07:40:14.762745Z","shell.execute_reply.started":"2023-12-14T07:38:14.246195Z","shell.execute_reply":"2023-12-14T07:40:14.761778Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ec581c6384f4c318a766a16cb6eb791"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)fetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6edd41019be4306b4cf2a430f019e1c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf40949ab4a34387b927f03bdb7d3cd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f4cf62a55164784b112d418284d1085"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb300c0fb1a946d3a21676cfdd115004"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"857dbab43b7f4ac6b9df5e7a5fb9d6ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dab7608069a43b5962c55a4449616f2"}},"metadata":{}},{"name":"stdout","text":"trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.04703666202518836\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Chat","metadata":{}},{"cell_type":"code","source":"def chat():\n    while True:\n        question = input('You:')\n        if question.lower() == 'end chat':\n            print('Thank you for chatting with me. I hope you have a great day!')\n            break\n        else:\n            with torch.no_grad():\n                model.eval()\n                text = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{question}\\n\\n### Response:\\n\"\"\"\n                answer_ids =  model.backbone.generate(\n                        **tokenizer(text, return_tensors='pt').to(device),\n                        max_new_tokens=1000,\n                        pad_token_id=tokenizer.pad_token_id,\n                )[0]\n                start_idx = torch.where(answer_ids == 12107)[0] + 2\n                print('Bot:', tokenizer.decode(answer_ids[start_idx:-1]))\n                print('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-12-14T07:40:14.764129Z","iopub.execute_input":"2023-12-14T07:40:14.764430Z","iopub.status.idle":"2023-12-14T07:40:14.771942Z","shell.execute_reply.started":"2023-12-14T07:40:14.764404Z","shell.execute_reply":"2023-12-14T07:40:14.770969Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"chat()","metadata":{"execution":{"iopub.status.busy":"2023-12-14T07:40:14.773067Z","iopub.execute_input":"2023-12-14T07:40:14.773382Z","iopub.status.idle":"2023-12-14T07:44:52.279972Z","shell.execute_reply.started":"2023-12-14T07:40:14.773358Z","shell.execute_reply":"2023-12-14T07:44:52.278929Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdin","text":"You: Tell me about AI Winter\n"},{"name":"stdout","text":"Bot: \nAI Winter is a period of time when the field of artificial intelligence (AI) experiences a decline in funding and interest. This can be due to a number of factors, such as a lack of tangible results, a shift in public opinion, or a change in the economic climate. During an AI Winter, researchers and developers may turn to other fields or pursue other interests, and the progress of AI may slow down or even stall. However, AI Winters are not permanent, and they are often followed by periods of renewed interest and investment in the field.\n\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You: Give me code example for image classification in PyTorch \n"},{"name":"stdout","text":"Bot: \nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\n# Load the dataset\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n\n# Define the model\nmodel = torchvision.models.resnet18(pretrained=True)\n\n# Define the loss function and optimizer\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Train the model\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n# Evaluate the model\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data, target in test_loader:\n        data, target = data.to(device), target.to(device)\n        output = model(data)\n        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n        total += target.size(0)\n        correct += pred.eq(target.view_as(pred)).sum().item()\n\naccuracy = 100 * correct / total\nprint('Accuracy: {} %'.format(accuracy))\n\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"You: end chat\n"},{"name":"stdout","text":"Thank you for chatting with me. I hope you have a great day!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}